{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11826c331dbfea11"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:51.689980Z",
     "start_time": "2024-01-11T13:40:51.688533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2\n",
      "torchvision version: 0.16.2\n"
     ]
    }
   ],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torch import nn\n",
    "\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(\n",
    "            torch.__version__.split(\".\")[0]\n",
    "            ) == 2, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https: // download.pytorch.org/whl/cu118\n",
    "    import torch\n",
    "    import torchvision\n",
    "\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https: // github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular.\n",
    "    !mv pytorch-deep-learning/helper_functions.py.  # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device choser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540059a5025578a0"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def device_chooser(prefer_device: str = \"cpu\") -> str:\n",
    "    devices = {}\n",
    "    if torch.cuda.is_available():\n",
    "        devices[\"cuda\"] = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        devices[\"mps\"] = \"mps\"\n",
    "    else:\n",
    "        devices[\"cpu\"] = \"cpu\"\n",
    "\n",
    "    if prefer_device in devices:\n",
    "        return devices[prefer_device]\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "device = device_chooser(prefer_device=\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:52.575512Z",
     "start_time": "2024-01-11T13:40:52.571629Z"
    }
   },
   "id": "57b47f7d5e82da6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4cdd1ee037773c"
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": "PosixPath('data/pizza_steak_sushi')"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = download_data(\n",
    "        source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/pizza_steak_sushi.zip\",\n",
    "        destination=Path(\"pizza_steak_sushi\")\n",
    "        )\n",
    "image_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:52.972940Z",
     "start_time": "2024-01-11T13:40:52.964654Z"
    }
   },
   "id": "8e97ac6a4340765a"
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "train_path = image_path / \"train\"\n",
    "test_path = image_path / \"test\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:53.165661Z",
     "start_time": "2024-01-11T13:40:53.152980Z"
    }
   },
   "id": "a682316b33edfad0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset | dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddff74f33caa047e"
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "HEIGHT, WIDTH = 224, 224\n",
    "IMG_SIZE = (HEIGHT, WIDTH)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "manual_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(IMG_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=manual_transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path, transform=manual_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:53.516126Z",
     "start_time": "2024-01-11T13:40:53.509905Z"
    }
   },
   "id": "a4c542fcbbc253a2"
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=768,\n",
    "        kernel_size=PATCH_SIZE,\n",
    "        stride=PATCH_SIZE,\n",
    "        padding=0\n",
    "        )\n",
    "\n",
    "flatten = nn.Flatten(start_dim=1, end_dim=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T13:40:53.705878Z",
     "start_time": "2024-01-11T13:40:53.698577Z"
    }
   },
   "id": "d0ad1e00bca47e86"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([768, 14, 14])\n",
      "torch.Size([768, 196])\n"
     ]
    }
   ],
   "source": [
    "im_batch: torch.Tensor = next(iter(train_dataloader))[0]\n",
    "im: torch.Tensor = im_batch[0]\n",
    "\n",
    "print(im.shape)\n",
    "image_embeddings: torch.Tensor = conv2d(im)\n",
    "flattened_image_embeddings: torch.Tensor = flatten(image_embeddings)\n",
    "print(image_embeddings.shape, flattened_image_embeddings.shape, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T11:50:00.570043Z",
     "start_time": "2024-01-11T11:50:00.442078Z"
    }
   },
   "id": "233195485f5cf24e"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 224, 224])"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_batch, _ = next(iter(train_dataloader))\n",
    "im: torch.Tensor = im_batch[0].unsqueeze(0)\n",
    "im.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T12:28:05.720107Z",
     "start_time": "2024-01-11T12:28:05.584722Z"
    }
   },
   "id": "c5994dc05da07199"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patcher = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=embed_dim,\n",
    "                        stride=patch_size,\n",
    "                        kernel_size=patch_size,\n",
    "                        padding=0\n",
    "                        ),\n",
    "                nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % self.patch_size == 0, f\"[ERROR] - Image resolution is not divisible by patch size. Image size {x.shape} | Patch size {self.patch_size}\"\n",
    "\n",
    "        return self.patcher(x).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "set_seeds()\n",
    "patchify = PatchEmbedding(in_channels=3, patch_size=PATCH_SIZE, embed_dim=768)\n",
    "patch_embedded_image = patchify(im)\n",
    "\n",
    "print(\n",
    "        f\"{im.shape}\",\n",
    "        f\"{patchify(im).shape}\",\n",
    "        sep=\"\\n\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T12:34:27.829049Z",
     "start_time": "2024-01-11T12:34:27.781377Z"
    }
   },
   "id": "e4d70c06edc84681"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([1, 197, 768]), torch.Size([1, 197, 768]))"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 768\n",
    "class_token = nn.Parameter(\n",
    "        torch.randn(1, 1, embedding_dimension), requires_grad=True\n",
    "        )\n",
    "\n",
    "print(class_token.shape, patch_embedded_image.shape, sep=\"\\n\")\n",
    "\n",
    "patch_embedded_image_with_class_embedding = torch.cat(\n",
    "        (class_token, patch_embedded_image), dim=1\n",
    "        )\n",
    "patch_position_embeddings = nn.Parameter(\n",
    "        torch.randn(1, 197, embedding_dimension),\n",
    "        requires_grad=True\n",
    "        )\n",
    "\n",
    "position_and_class_embeddings = (\n",
    "        patch_embedded_image_with_class_embedding + patch_position_embeddings)\n",
    "\n",
    "patch_embedded_image_with_class_embedding.shape, position_and_class_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T12:47:41.926251Z",
     "start_time": "2024-01-11T12:47:41.886917Z"
    }
   },
   "id": "c82be7bad4eb4bae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
