{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11826c331dbfea11"
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:26:53.496247Z",
     "start_time": "2024-01-15T11:26:53.464054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2\n",
      "torchvision version: 0.16.2\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torch import nn\n",
    "\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(\n",
    "            torch.__version__.split(\".\")[0]\n",
    "            ) == 2, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https: // download.pytorch.org/whl/cu118\n",
    "    import torch\n",
    "    import torchvision\n",
    "\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https: // github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular.\n",
    "    !mv pytorch-deep-learning/helper_functions.py.  # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device choser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540059a5025578a0"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "'mps'"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def device_chooser(prefer_device: str = \"cpu\") -> str:\n",
    "    devices = {}\n",
    "    if torch.cuda.is_available():\n",
    "        devices[\"cuda\"] = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        devices[\"mps\"] = \"mps\"\n",
    "    else:\n",
    "        devices[\"cpu\"] = \"cpu\"\n",
    "\n",
    "    if prefer_device in devices:\n",
    "        return devices[prefer_device]\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "device = device_chooser(prefer_device=\"mps\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:27:02.828642Z",
     "start_time": "2024-01-15T11:27:02.815883Z"
    }
   },
   "id": "57b47f7d5e82da6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4cdd1ee037773c"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": "PosixPath('data/pizza_steak_sushi')"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = download_data(\n",
    "        source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/pizza_steak_sushi.zip\",\n",
    "        destination=Path(\"pizza_steak_sushi\")\n",
    "        )\n",
    "image_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:27:06.250374Z",
     "start_time": "2024-01-15T11:27:06.243628Z"
    }
   },
   "id": "8e97ac6a4340765a"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "train_path = image_path / \"train\"\n",
    "test_path = image_path / \"test\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:27:10.503215Z",
     "start_time": "2024-01-15T11:27:10.492074Z"
    }
   },
   "id": "a682316b33edfad0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset | dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddff74f33caa047e"
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "HEIGHT, WIDTH = 224, 224\n",
    "IMG_SIZE = (HEIGHT, WIDTH)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "manual_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(IMG_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=manual_transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path, transform=manual_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:27:12.277809Z",
     "start_time": "2024-01-15T11:27:12.266741Z"
    }
   },
   "id": "a4c542fcbbc253a2"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=768,\n",
    "        kernel_size=PATCH_SIZE,\n",
    "        stride=PATCH_SIZE,\n",
    "        padding=0\n",
    "        )\n",
    "\n",
    "flatten = nn.Flatten(start_dim=1, end_dim=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:27:19.864371Z",
     "start_time": "2024-01-15T11:27:19.850530Z"
    }
   },
   "id": "d0ad1e00bca47e86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Patch embeding | class token | position embeding "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10c448580a9e4a3f"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([768, 14, 14])\n",
      "torch.Size([768, 196])\n"
     ]
    }
   ],
   "source": [
    "im_batch: torch.Tensor = next(iter(train_dataloader))[0]\n",
    "im: torch.Tensor = im_batch[0]\n",
    "\n",
    "print(im.shape)\n",
    "image_embeddings: torch.Tensor = conv2d(im)\n",
    "flattened_image_embeddings: torch.Tensor = flatten(image_embeddings)\n",
    "print(image_embeddings.shape, flattened_image_embeddings.shape, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:29:36.812151Z",
     "start_time": "2024-01-15T11:29:36.631385Z"
    }
   },
   "id": "233195485f5cf24e"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 224, 224])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_batch, _ = next(iter(train_dataloader))\n",
    "im: torch.Tensor = im_batch[0].unsqueeze(0)\n",
    "im.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:29:41.940917Z",
     "start_time": "2024-01-15T11:29:41.815279Z"
    }
   },
   "id": "c5994dc05da07199"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int, patch_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patcher = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=embed_dim,\n",
    "                        stride=patch_size,\n",
    "                        kernel_size=patch_size,\n",
    "                        padding=0\n",
    "                        ),\n",
    "                nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % self.patch_size == 0, f\"[ERROR] - Image resolution is not divisible by patch size. Image size {x.shape} | Patch size {self.patch_size}\"\n",
    "\n",
    "        return self.patcher(x).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "set_seeds()\n",
    "patchify = PatchEmbedding(in_channels=3, patch_size=PATCH_SIZE, embed_dim=768)\n",
    "patch_embedded_image = patchify(im)\n",
    "\n",
    "print(\n",
    "        f\"{im.shape}\",\n",
    "        f\"{patchify(im).shape}\",\n",
    "        sep=\"\\n\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:29:44.343630Z",
     "start_time": "2024-01-15T11:29:44.327979Z"
    }
   },
   "id": "e4d70c06edc84681"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([1, 197, 768]), torch.Size([1, 197, 768]))"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 768\n",
    "class_token = nn.Parameter(\n",
    "        torch.randn(1, 1, embedding_dimension), requires_grad=True\n",
    "        )\n",
    "\n",
    "print(class_token.shape, patch_embedded_image.shape, sep=\"\\n\")\n",
    "\n",
    "patch_embedded_image_with_class_embedding = torch.cat(\n",
    "        (class_token, patch_embedded_image), dim=1\n",
    "        )\n",
    "patch_position_embeddings = nn.Parameter(\n",
    "        torch.randn(1, 197, embedding_dimension),\n",
    "        requires_grad=True\n",
    "        )\n",
    "\n",
    "position_and_class_embeddings = (\n",
    "        patch_embedded_image_with_class_embedding + patch_position_embeddings)\n",
    "\n",
    "patch_embedded_image_with_class_embedding.shape, position_and_class_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T11:29:49.704238Z",
     "start_time": "2024-01-15T11:29:49.677185Z"
    }
   },
   "id": "c82be7bad4eb4bae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-head attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a535d535f1b4e7a2"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, attn_dropout: float | int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=attn_dropout,\n",
    "                batch_first=True\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x: torch.Tensor = self.layer_norm(x)\n",
    "        return self.multihead_attn(query=x, key=x, value=x, need_weights=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T12:18:46.804149Z",
     "start_time": "2024-01-15T12:18:46.800846Z"
    }
   },
   "id": "add653a5cc18ca95"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 197, 768])"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_block = MultiHeadSelfAttentionBlock(embed_dim=embedding_dimension, num_heads=12, attn_dropout=0)\n",
    "\n",
    "patch_image_after_msa = msa_block(position_and_class_embeddings)\n",
    "patch_image_after_msa.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T12:19:07.452127Z",
     "start_time": "2024-01-15T12:19:07.429318Z"
    }
   },
   "id": "8bc3e364fc41b9f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multilayer Perceptron (MLP)¶"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3185087eb6f2abd"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = embedding_dimension, mlp_dim: int = 3072, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(in_features=embedding_dim, out_features=mlp_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(in_features=mlp_dim, out_features=embedding_dim),\n",
    "                nn.Dropout(p=dropout)\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x: torch.Tensor = self.layer_norm(x)\n",
    "        return self.mlp(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T12:34:35.607557Z",
     "start_time": "2024-01-15T12:34:35.599409Z"
    }
   },
   "id": "26db79c8893de723"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 197, 768])"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_block = MLPBlock()\n",
    "\n",
    "patch_image_through_mlp_block: torch.Tensor = mlp_block(patch_image_after_msa)\n",
    "patch_image_through_mlp_block.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T12:38:21.681225Z",
     "start_time": "2024-01-15T12:38:21.647347Z"
    }
   },
   "id": "4432817db085bdbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Transformer Encoder by combining our custom made layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540c5d54c65e67b3"
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int = 768,  # Hidden size D from Table 1 for ViT-Base\n",
    "            num_heads: int = 12,  # Heads from Table 1 for ViT-Base\n",
    "            mlp_size: int = 3072,  # MLP size from Table 1 for ViT-Base\n",
    "            mlp_dropout: float = 0.1,  # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "            attn_dropout: float = 0\n",
    "            ):  # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiHeadSelfAttentionBlock(\n",
    "                embed_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                attn_dropout=attn_dropout\n",
    "                )\n",
    "\n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block = MLPBlock(\n",
    "                embedding_dim=embedding_dim,\n",
    "                mlp_dim=mlp_size,\n",
    "                dropout=mlp_dropout\n",
    "                )\n",
    "\n",
    "    # 5. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x = self.msa_block(x) + x\n",
    "\n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T13:02:11.499034Z",
     "start_time": "2024-01-15T13:02:11.490176Z"
    }
   },
   "id": "475580c50a216622"
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTransformerEncoderBlock                  [1, 197, 768]             --\n├─MultiHeadSelfAttentionBlock: 1-1       [1, 197, 768]             --\n│    └─LayerNorm: 2-1                    [1, 197, 768]             1,536\n│    └─MultiheadAttention: 2-2           [1, 197, 768]             2,362,368\n├─MLPBlock: 1-2                          [1, 197, 768]             --\n│    └─LayerNorm: 2-3                    [1, 197, 768]             1,536\n│    └─Sequential: 2-4                   [1, 197, 768]             --\n│    │    └─Linear: 3-1                  [1, 197, 3072]            2,362,368\n│    │    └─GELU: 3-2                    [1, 197, 3072]            --\n│    │    └─Dropout: 3-3                 [1, 197, 3072]            --\n│    │    └─Linear: 3-4                  [1, 197, 768]             2,360,064\n│    │    └─Dropout: 3-5                 [1, 197, 768]             --\n==========================================================================================\nTotal params: 7,087,872\nTrainable params: 7,087,872\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 4.73\n==========================================================================================\nInput size (MB): 0.61\nForward/backward pass size (MB): 8.47\nParams size (MB): 18.90\nEstimated Total Size (MB): 27.98\n=========================================================================================="
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "summary(transformer_encoder_block, (1, 197, 768))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T13:02:33.253447Z",
     "start_time": "2024-01-15T13:02:33.192897Z"
    }
   },
   "id": "fbe71ae516988c9"
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "data": {
      "text/plain": "==============================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #\n==============================================================================================================\nTransformerEncoderLayer (TransformerEncoderLayer)  [1, 197, 768]        [1, 197, 768]        7,087,872\n==============================================================================================================\nTotal params: 7,087,872\nTrainable params: 7,087,872\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0\n==============================================================================================================\nInput size (MB): 0.61\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.61\n=============================================================================================================="
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embedding_dimension,\n",
    "        nhead=12,\n",
    "        dim_feedforward=3072,\n",
    "        dropout=0.1,\n",
    "        activation=nn.GELU(),\n",
    "        batch_first=True,\n",
    "        norm_first=True\n",
    "        )\n",
    "\n",
    "summary(model=torch_transformer_encoder_layer, input_size=(1, 197, 768), col_names=[\"input_size\", \"output_size\", \"num_params\"], col_width=20, row_settings=[\"var_names\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T13:08:48.185406Z",
     "start_time": "2024-01-15T13:08:48.130012Z"
    }
   },
   "id": "16ee304b5746c16e"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n  (activation): GELU(approximate='none')\n)"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_transformer_encoder_layer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T13:18:07.866074Z",
     "start_time": "2024-01-15T13:18:07.860456Z"
    }
   },
   "id": "593e9845d05a7414"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc6402ab46910af"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "\n",
    "        # 3. Make the image size is divisble by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1)\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T13:19:59.748996Z",
     "start_time": "2024-01-15T13:19:59.709308Z"
    }
   },
   "id": "a2dc0b037138eb30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9c14b3e2bcd82f2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
